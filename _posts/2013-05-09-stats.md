---
layout: post
title: Hailo Stats
tagline: The one where they talk about stats
github: domwong
image: https://0.gravatar.com/avatar/a2be4a7b3957556a1dae9611ee42b8f8?d=https%3A%2F%2Fidenticons.github.com%2F895f7677fc70a189de94897f84f54acc.png
categories: data metrics java
---
Here at Hailo our bread and butter is matching customers to taxis which sounds pretty straight forward and, in truth, if you approach it as a simple match making service then it is. Then again, if that's all we did we could have wrapped this all up a long time ago and I could be lying on a beach somewhere drinking mojitos. Fortunately for my liver, our approach is a little more sophisticated and we're always looking for ways to improve our service. "But how do you improve on a service as perfect as Hailo?" I hear you ask. You need two things:

1. Data
2. More data

The more data you have the better equipped you are to make informed decisions about how to do things better. Whether that data is about your system, your users, your environment or something else, each additional data point is valuable. This has been our mantra from day one, so we're constantly recording stats about various parts of our system and its operation and always looking for new stats that we can generate to give us more insight. How much data are we collecting? We're currently collecting around ***50 million data points per day*** but this will inevitably grow as we roll out to more cities and grow in our existing ones. Since we went live in late 2011 we've collected 10's of TBs of data. So how did we approach this problem of gathering data? 

## What and Why?

First, you need to define what you're interested in. Every time something happens in our system that we think is interesting we'll generate an **event**. An event is just a bunch of key/value pairs, the only two mandatory ones are

- *timestamp* so we know when the event happened
- *eventType* so we know what kind of event happened

Then we add a bunch of other information of interest depending on what event has happened, like GPS positions etc. What kind of events are we interested in? All sorts, for example:

- When/Where a customer hits "Pick Me Up Here"
- When/Where a driver accepts/declines a job 
- When a driver goes on shift
- When/Where a customer registers

But why are we interested in data like this? Well, just off the bat we could:

- Predict where and when customer demand will be high - Saturday night as clubs are closing, Monday night after the big concert etc. We could then tell drivers where the hotspots will be and tell passengers when they'll have a better chance of scoring a ride home.
- Monitor our driver supply at various times of the day - when do we have the best coverage to serve passengers, but also when do we have the worst. Do some things to help make the supply better match the demand.
- Work out what pieces of PR drive the most new customer registrations

The possibilities are only limited by our imagination (unfortunately my imagination has been stunted by TV and the internet).

## How do we collect this data?

Now we know what data we'd like to have, how do we go about getting it? 

We created a centralised stats service into which all parts of the system can fire events. The service at its most basic does the following:

1. Accept an event as JSON (this can be via either HTTP POST request or message queue)
2. Do some preprocessing of the data - calculate geohashes etc
3. Store the event in our data store

Simple. This simplicity makes it very fast both to process but also to add new events when we identify them. The technologies we use also make it easily scalable. The service is a simple Java app deployed on Tomcat and using Cassandra as its data store. It is stateless so we can add more nodes to horizontally scale; we're currently running 6 nodes across the world to provide extra resiliency. As our storage needs increase we can also horizontally scale our Cassandra cluster so no problems there either. Also, Cassandra is pretty darn quick for writes which is pretty useful for us especially as we ramp up both the volume and variety of stats that we collect.

## Great. Now what do we do with it all?

It's one thing gathering the data but it's another working out what to do with it all. 

### Getting the data 

Firstly, we need to get it back out. We have a couple of different ways to get to the data:

- We have a number of endpoints which can slice and dice the data, allowing it to be queried by various different dimensions. These are queried by a number of different systems which pull in the data for further data analysis.
- Every event that we collect, once it's stored safely in Cassandra, is exposed as a stream of data which anyone can subscribe to. Something like Twitter's firehose but for our internal stats events. We use NSQ to expose this data stream; (see [here](https://github.com/bitly/nsq)) an open source message processing system from the team at bit.ly. So accessing this data is as simple as subscribing to the stream.


### Getting insight A.K.A the payoff

Once we've got the data out we need to analyse it and we've got a few different systems into which we can feed the data. One notable one is [Acunu Analytics](http://www.acunu.com/) which does a great job of real time aggregation of data and fits in nicely with our infrastructure since it makes heavy use of Cassandra. Whatever tool you use for analysis though, is only as good as the questions that you're asking and it's down to our data scientists and other interested parties to ask the most interesting questions to give us the best insights.

